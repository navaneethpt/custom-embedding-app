# Custom Domain-Specific Embedding System

A full-stack application demonstrating custom embedding generation using Siamese neural networks for domain-specific semantic search. This project implements the concepts from [this blog post](https://navaneethpt.tech/posts/custom_embedding/).

## ğŸ¯ Overview

This application allows you to:
- Train custom word embeddings on domain-specific documents
- Compare similarity between words using your custom-trained model
- Compare results against generic HuggingFace embeddings
- Visualize the effectiveness of domain-specific embeddings

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React UI      â”‚
â”‚  (Frontend)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ REST API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spring Boot    â”‚
â”‚    Backend      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  DJL Engine     â”‚
â”‚ (Siamese NN)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

1. **Backend (Java + DJL)**
   - Siamese neural network for custom embeddings
   - Document processing and training data generation
   - REST API endpoints
   - Integration with HuggingFace models for comparison

2. **Frontend (React)**
   - Document upload interface
   - Training control panel
   - Similarity comparison tool
   - Results visualization

## ğŸš€ Quick Start

### Prerequisites

- Java 17 or higher
- Node.js 16 or higher
- Maven 3.6+
- npm or yarn

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/custom-embedding-app.git
   cd custom-embedding-app
   ```

2. **Add your documents**
   ```bash
   # Place your .txt documents in the documents folder
   cp your-documents/*.txt backend/src/main/resources/documents/
   ```

3. **Start the backend**
   ```bash
   cd backend
   mvn clean install
   mvn spring-boot:run
   ```
   Backend will start on `http://localhost:8080`

4. **Start the frontend**
   ```bash
   cd frontend
   npm install
   npm start
   ```
   Frontend will start on `http://localhost:3000`

## ğŸ“– How It Works

### 1. Training Data Generation

The system automatically:
- Reads documents from the `documents` folder
- Tokenizes content into sentences
- Builds word pair correlations based on proximity
- Generates similarity labels (1.0 for adjacent words, decreasing with distance)

Example:
```
Sentence: "AWS S3 object storage"
Pairs: {aws,s3,1.0}, {aws,object,0.5}, {s3,object,1.0}, {s3,storage,0.5}
```

### 2. Siamese Network Architecture

```
Input (one-hot) â†’ Linear(vocabâ†’32) â†’ ReLU â†’ Linear(32â†’embedDim) â†’ Embedding
```

Both words in a pair pass through the **same network** (weight sharing).

### 3. Contrastive Loss

```
loss = y Ã— distÂ² + (1-y) Ã— max(margin - dist, 0)Â²
```

Where:
- `y`: similarity label (0-1)
- `dist`: Euclidean distance between embeddings
- `margin`: minimum distance for dissimilar pairs (default: 2.0)

### 4. Embedding & Similarity

Once trained, words are embedded using the network:
```java
NDArray embedding = predictor.predict(oneHot(word));
similarity = cosineSimilarity(embed1, embed2);
```

## ğŸ® Usage

### Training the Model

1. Navigate to `http://localhost:3000`
2. Click **"Train Model"** button
3. Wait for training to complete (progress shown in UI)
4. Vocabulary and training stats will be displayed

### Comparing Similarities

1. Enter two words from your domain vocabulary
2. Click **"Calculate Similarity"**
3. View results:
   - **Custom Model**: Your domain-specific similarity
   - **Generic Model**: HuggingFace embedding similarity
   - **Difference**: How much your custom model differs

### Example Use Case

**Documents**: Cloud computing documentation (AWS, Azure, GCP)

**Query**: "s3" vs "blob-storage"

**Results**:
- Generic embedding: Low similarity (different terms)
- Custom embedding: High similarity (both are object storage services)

## ğŸ“Š API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/api/train` | Train the embedding model |
| GET | `/api/status` | Get training status |
| POST | `/api/similarity/custom` | Calculate custom similarity |
| POST | `/api/similarity/generic` | Calculate generic similarity |
| POST | `/api/similarity/compare` | Compare both similarities |
| GET | `/api/vocabulary` | Get current vocabulary |

## ğŸ”§ Configuration

### Backend (`application.properties`)

```properties
# DJL Configuration
embedding.dimension=16
embedding.margin=2.0
embedding.epochs=300
embedding.learning-rate=0.01

# Document Processing
documents.folder=src/main/resources/documents
documents.max-distance=5
```

### Frontend (`.env`)

```env
REACT_APP_API_URL=http://localhost:8080/api
```

## ğŸ“ Project Structure

```
custom-embedding-app/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ src/main/java/org/example/ml/
â”‚   â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”‚   â””â”€â”€ SiameseEmbedding.java
â”‚   â”‚   â”œâ”€â”€ service/
â”‚   â”‚   â”‚   â”œâ”€â”€ EmbeddingService.java
â”‚   â”‚   â”‚   â”œâ”€â”€ DocumentProcessor.java
â”‚   â”‚   â”‚   â””â”€â”€ HuggingFaceService.java
â”‚   â”‚   â”œâ”€â”€ controller/
â”‚   â”‚   â”‚   â””â”€â”€ EmbeddingController.java
â”‚   â”‚   â””â”€â”€ dto/
â”‚   â”‚       â””â”€â”€ SimilarityRequest.java
â”‚   â”œâ”€â”€ src/main/resources/
â”‚   â”‚   â””â”€â”€ documents/          # Place your .txt files here
â”‚   â””â”€â”€ pom.xml
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ TrainingPanel.jsx
â”‚   â”‚   â”‚   â”œâ”€â”€ SimilarityChecker.jsx
â”‚   â”‚   â”‚   â””â”€â”€ ResultsDisplay.jsx
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â””â”€â”€ apiService.js
â”‚   â”‚   â””â”€â”€ App.js
â”‚   â””â”€â”€ package.json
â””â”€â”€ README.md
```

## ğŸ§ª Testing

```bash
# Backend tests
cd backend
mvn test

# Frontend tests
cd frontend
npm test
```

## ğŸ”¬ Technical Details

### Why Custom Embeddings?

Generic embeddings (Word2Vec, BERT, etc.) are trained on general corpora and may not capture domain-specific relationships:

**Problem**: Generic models don't know that "EC2" and "virtual-machine" are highly related in cloud computing context.

**Solution**: Train embeddings on your specific domain where proximity in text indicates semantic relationship.

### Siamese Network Benefits

1. **Weight Sharing**: Same network for all inputs â†’ learns consistent representation
2. **Contrastive Learning**: Explicitly learns what's similar vs dissimilar
3. **Flexible**: Works with small datasets (100s of pairs)
4. **Domain-Adaptive**: Captures your specific similarity logic

### Comparison with Generic Models

| Aspect | Custom Embedding | Generic Embedding |
|--------|-----------------|-------------------|
| Training Data | Your documents | Internet-scale corpus |
| Vocabulary | Domain-specific | General language |
| Similarity Logic | Your definitions | Statistical co-occurrence |
| Size | Small (KB) | Large (GB) |
| Inference | Fast | Slower |

## ğŸš§ Limitations

- Requires labeled similarity data (auto-generated from proximity)
- Limited to vocabulary seen during training
- Quality depends on document quality and diversity
- Not suitable for general-purpose language tasks

## ğŸ› ï¸ Extending the Project

### Add More Features

1. **Batch Processing**: Process multiple document sets
2. **Model Versioning**: Save/load different trained models
3. **Visualization**: t-SNE plots of embedding space
4. **API Authentication**: Secure the endpoints
5. **Vector Database**: Integrate with Pinecone/Weaviate for large-scale search

### Improve Training

1. **Data Augmentation**: Synonym replacement, paraphrasing
2. **Triplet Loss**: Use triplet loss instead of contrastive
3. **Hard Negative Mining**: Focus on difficult examples
4. **Cross-Validation**: Validate on held-out pairs

## ğŸ“š References

- [Original Blog Post](https://navaneethpt.tech/posts/custom_embedding/)
- [DJL Documentation](https://djl.ai/)
- [Siamese Networks](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)
- [Contrastive Learning](https://arxiv.org/abs/2002.05709)

## ğŸ“ License

MIT License - feel free to use this project for learning and commercial purposes.

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

## ğŸ‘¤ Author

Navaneeth P T - [https://github.com/navaneethpt](https://github.com/yourusername)

## ğŸ™ Acknowledgments

- DJL (Deep Java Library) team
- HuggingFace for pre-trained models
